{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"0fad6c31296b463ba678516edd6c48b8","deepnote_cell_type":"text-cell-h1","formattedRanges":[]},"source":["# Team 7UP"]},{"cell_type":"markdown","metadata":{"cell_id":"3eea577f60ab4eaea478b1f1442c90ad","deepnote_cell_type":"text-cell-h2","formattedRanges":[]},"source":["## SDS Datathon 2024 Champions Group"]},{"cell_type":"markdown","metadata":{"cell_id":"c8ba56d4597c4c60991241375cfbef1d","deepnote_cell_type":"text-cell-h3","formattedRanges":[]},"source":["### By Winston, Maximus, Jian Hao, Caleb, Ryan"]},{"cell_type":"markdown","metadata":{"cell_id":"a796d4835f7842b09160afc081780eeb","deepnote_cell_type":"markdown"},"source":["##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n","##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"92e25c68604e48c2ae5683957269ff6f","deepnote_cell_type":"code"},"outputs":[],"source":["#%pip install pandas \n","#%pip install matplotlib\n","# add commented pip installation lines for packages used as shown above for ease of testing\n","# the line should be of the format %pip install PACKAGE_NAME \n","%pip install pandas\n","%pip install numpy\n","%pip install matplotlib\n","%pip install seaborn\n","%pip install scipy\n","%pip install xgboost\n","%pip install kmodes"]},{"cell_type":"code","execution_count":2,"metadata":{"cell_id":"10ae4c5884d5453f89d878b47f4216f5","deepnote_cell_type":"code"},"outputs":[],"source":["# Import relevant packages\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import scipy.stats"]},{"cell_type":"markdown","metadata":{"cell_id":"94adcd3cd3a540b095b36639dd2d561d","deepnote_cell_type":"markdown"},"source":["## **DO NOT CHANGE** the filepath variable\n","##### Instead, create a folder named 'data' in your current working directory and \n","##### have the .csv file inside that. A relative path *must* be used when loading data into pandas"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"36a2284aa51b4d2382758d183569b686","deepnote_cell_type":"code"},"outputs":[],"source":["filepath = \"./data/catA_train.csv\" \n","# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file\n","\n","df = pd.read_csv(filepath)\n","df2 = df.copy()"]},{"cell_type":"markdown","metadata":{"cell_id":"02121fa4925f4ba0b2025d7fa58a7ff7","deepnote_cell_type":"markdown"},"source":["### **ALL** Code for machine learning and dataset analysis should be entered below. \n","##### Ensure that your code is clear and readable.\n","##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."]},{"cell_type":"markdown","metadata":{"cell_id":"db762ee42aea4885b7f901c629b1b53b","deepnote_cell_type":"markdown"},"source":["### **Part 0: Background Information**"]},{"cell_type":"markdown","metadata":{"cell_id":"2cdbbf23e27940f7adacb53f86401bb2","deepnote_cell_type":"markdown"},"source":["**0.1. General Information**"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ec8dc938e5394277ac85c494dac779cc","deepnote_cell_type":"code"},"outputs":[],"source":["print(df.info())"]},{"cell_type":"markdown","metadata":{"cell_id":"98b34b61e6d5476b99efbb5c2efcbae4","deepnote_cell_type":"markdown"},"source":["**0.2. Unique Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"6a4ebbcb480c4af1964a590b47ac3819","deepnote_cell_type":"code"},"outputs":[],"source":["# Total number of unique values for each column\n","print(df.nunique())"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"3e3067b581c54f6ba3373a692e18ec7b","deepnote_cell_type":"code"},"outputs":[],"source":["# Unique value in Company Status (Active/Inactive)\n","print(df['Company Status (Active/Inactive)'].unique())"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8f5a95ce6c0943d2a68eacc7381b0ad2","deepnote_cell_type":"code"},"outputs":[],"source":["# Number of occurences of unique values in Ownership Type\n","print(df[\"Ownership Type\"].value_counts())"]},{"cell_type":"markdown","metadata":{"cell_id":"cd902b28b0e74283b34f7086bda19bea","deepnote_cell_type":"markdown"},"source":["**0.3. Modal Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"62ca64649de747658e9fd8e7223e43e3","deepnote_cell_type":"code"},"outputs":[],"source":["# Percentage occurrence of modal values for each column\n","pd.DataFrame({'Columns': df.columns, 'Val (%)': [((df[x].isin(df[x].mode()).sum() / df.shape[0]) * 100).round(1)  for x in df]})"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a74a3b6ad7724f73b92f1b6e03ec55d1","deepnote_cell_type":"code"},"outputs":[],"source":["# pd.DataFrame({'Columns': df.columns, 'Val':[df[x].isin(df[x].mode()).sum() for x in df]})\n","\n","value_counts = df['Parent Company'].value_counts()\n","\n","values_appear_once = value_counts[value_counts <= 1]\n","print(values_appear_once)\n","# count_values_appear_once = len(values_appear_once)\n","# print(\"Number of values appearing only once:\", count_values_appear_once)"]},{"cell_type":"markdown","metadata":{"cell_id":"4ab3bfac7c8b468ab0b94a6e24ee77d3","deepnote_cell_type":"markdown"},"source":["**0.4. Missing Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7758efd950b544c083b9629a86be03d8","deepnote_cell_type":"code"},"outputs":[],"source":["# Total number of missing values for each column\n","print(df.isna().sum())"]},{"cell_type":"markdown","metadata":{"cell_id":"e20026f2b05d4fddb7421ceca2725a02","deepnote_cell_type":"markdown"},"source":["**0.5. Correlation Matrix for Variables**\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"4b59ffbe5ff14ad788089919d00eeeb9","deepnote_cell_type":"markdown"},"source":["In deriving the correlation matrix, we are required to: \n","\n","* Perform imputation for missing values.\n","\n","* Perform categorical encoding to convert non-numerical data to numerical data.\n","\n","(Why one-hot encoding over label encoding, or otherwise?)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"64729d0762d44be7b1e7a6cbf36b15ee","deepnote_cell_type":"code"},"outputs":[],"source":["# Imputation for missing values.\n","\n","# Perform one-hot encoding\n","df = pd.get_dummies(df, columns=['Entity Type'], prefix = 'Entity Type')\n","df = pd.get_dummies(df, columns=['Parent Country'], prefix='Parent Country')"]},{"cell_type":"markdown","metadata":{"cell_id":"918fd241825c4a3d877466ede35fa383","deepnote_cell_type":"markdown"},"source":["### **Part 1: Data Cleaning**"]},{"cell_type":"markdown","metadata":{"cell_id":"1d77151075a24ebab34233140cef2b04","deepnote_cell_type":"markdown"},"source":["Data cleaning involves identifying and rectifying errors and inconsistencies to enhance the overall quality and trustworthiness of the data. This is pivotal in improving model performance."]},{"cell_type":"markdown","metadata":{"cell_id":"6bc22745b01a4231bb7bcca79bda0bba","deepnote_cell_type":"markdown"},"source":["We perform the following processes as part of data cleaning: \n","* **1. Remove irrelevant columns.**\n","* **2. Remove irrelevant rows.**\n","\n","    The above removal processes are important as they ensure subsequent analysis is not slowed down or confused by irrelevant data.\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"0e003a9ff0734cc5af11e3ae6a9a417f","deepnote_cell_type":"markdown"},"source":["The following processes are also under Part 1 but have been done in 0.4. as they are instrumental in deriving the correlation matrix. Corresponding justifications are also provided in 0.4.\n","\n","* **Convert non-numerical data to numerical form.**\n","    \n","    This is important as it ensures machine models subsequently employed are able to interpret the data used, given these models can only interpret numerical values.\n","\n","* **Impute missing data with relevant values.**\n","\n","    This is important in ensuring completeness of datasets, reducing biasedness and inefficiency of statistical models.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"8a311cc2b13c48a7b29d3b9866e541ea","deepnote_cell_type":"markdown"},"source":["**1.1. Remove Irrelevant Columns**"]},{"cell_type":"markdown","metadata":{"cell_id":"04fdae86889f49ba9b7c622ad5a88db3","deepnote_cell_type":"markdown"},"source":["We remove columns with data deemed irrelevant due to the presence of the following:\n","\n","  * **1.1.1. Personally Identifiable Information (PII)**\n","  \n","    We remove the 2 columns for the attributes `AccountID` and `Company` as they constitute PII. PII is considered irrelevant in the case of sales forecasting because the focus is on aggregated and anonymized data rather than individual-level details. Sales forecasting involves analyzing trends, patterns, and historical data to make predictions about future sales performance at a broader level.\n","    \n"]},{"cell_type":"markdown","metadata":{"cell_id":"2f1bebb9590148b4af4ae4a3a5355b3e","deepnote_cell_type":"markdown"},"source":["  * **1.1.2. Redundant Descriptions**\n","\n","    We remove the 3 columns for the attributes **`8-Digit SIC Description`**, **`Industry`** and **`Company Description`** as they constitute redundant descriptions. \n","    \n","    `8-Digit SIC Description` and `Industry` are deemed redundant as they merely provide further information on `SIC Code` (which is already part of the DataFrame). We choose to retain `SIC Code` rather than the descriptions as `SIC Code` constitutes numerical data which is more easily interpreted by machine learning models.\n","    \n","    `Company Description` is deemed redundant as it merely provides further information on Company (which has already been removed in 1.1.1.)."]},{"cell_type":"markdown","metadata":{"cell_id":"0ba2f07c873e4c568e1389d6b9978f6b","deepnote_cell_type":"markdown"},"source":["  * **1.1.3. Insufficient Variability**\n","\n","    We remove the 2 columns for the attributes **`Company Status (Active/Inactive)`** and **`Ownership Type`** as the values are not sufficiently variable. Hence, the 2 columns do not provide valuable information for further analysis.\n","    \n","    In `Company Status (Active/Inactive)`, the value in each row is constant (i.e. \"Active\", according to 0.2.).\n","\n","    In `Ownership Type`, 96.8% of the values are \"Private\" (according to 0.2. and 0.3.), which suggests relatively limited variability. Moreover, none of the values rank in the top-quartile (i.e. top ...) when it comes to correlation with the target variable (Sales (Domestic Ultimate Total USD)). Hence, there is little ground to retain this column in the spirit of attempting to reduce the complexity of the model (prevent overfitting).\n","    \n","    \n","\n","    "]},{"cell_type":"markdown","metadata":{"cell_id":"09a4f92197af4f579da684d78799b3e8","deepnote_cell_type":"markdown"},"source":["  * **1.1.4. Excessive Missing Information**\n","\n","    We remove the column **`Square Footage`** as the data fields associated with the attribute are all empty (number of unique values in the column is 0 according to 0.2.). Thus, the column does not provide any information for analysis."]},{"cell_type":"markdown","metadata":{"cell_id":"198149d320ca4b62837d9450934a134a","deepnote_cell_type":"markdown"},"source":["  * **1.1.5. Excessive Number of Unique Values with Clustering Difficulties**\n","\n","    Columns of data with high cardinality (i.e. contain many unique values) should be considered for removal, in order to improve model performance. High cardinality in data results in sparser representation of each category in the dataset, which makes estimates of relationships between features and target variable less stable. \n","    \n","    Moreover, in the presence of high cardinality, overfitting may occur (i.e. model performs well on the training set but poorly on new data), as the model may memorize the training data and capturing specific instances rather than learning generalizable patterns. \n","\n","    We identify **`Parent Company`**, **`Global Ultimate Company`** and **`Domestic Ultimate Company`** as columns with high cardinality. Clustering to reduce cardinality is difficult as the companies represented in the columns are 17,882, 14,145 and 15,597 individual entities respectively (according to 0.2.) with a wide range of different characteristics. Hence, we decide to remove the above 3 columns.\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"ef0e5786975647b68f1bee7b293a32e4","deepnote_cell_type":"markdown"},"source":["* **1.1.6. Multicollinearity**\n","\n","    When there is multicollinearity (i.e. 2 attributes have a high degree of correlation), one of the attributes should be considered for removal. This is because the information from the second attribute would be redundant should the first attribute remain in the dataset. Having redundant information in the model would not contribute significantly to predicting sales figures and could also lead to instability. \n","    \n","    Furthermore, high correlation between 2 attributes makes it difficult to interpret the individual impact of each attribute.\n","\n","    We define the presence of multicollinearity as the existence of an absolute(-valued) Pearson correlation coefficient of at least 0.8. Applying this criterion, we identify that `SIC Code` and `8-Digit SIC Code` are highly correlated, as shown in the code below. \n","    \n","    With the above in mind, we choose to remove the column **`8-Digit SIC Code`** rather than `SIC Code`. This is due to 2 reasons. Firstly, the `8-Digit SIC Code` is less correlated with domestic sales figures (i.e. correlation coefficient is of smaller magnitude, 1). Secondly, there are more unique values for `8-Digit SIC Code`, which entails thie issues of high cardinality discussed in 1.1.5.\n","\n","    Furthermore, we will set a threshold on the SIC codes, only leaving in those that have more than 7 occurences, but labeling the rest as \"Others\"."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d782ec31420f4a88a7f91d963bb3f603","deepnote_cell_type":"code"},"outputs":[],"source":["from scipy.stats import pearsonr\n","\n","# Calculate the Pearson's correlation coefficient\n","correlation, p_value = pearsonr(df2['SIC Code'], df2['8-Digit SIC Code'])\n","\n","# Print the correlation coefficient\n","print(f\"Pearson's correlation coefficient: {correlation}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1a361bf31d4444d1978909e2aa21083e","deepnote_cell_type":"code"},"outputs":[],"source":["# Find the frequency of each industry\n","sic_code_frequency = df['SIC Code'].value_counts()\n","\n","# Set a threshold for low-frequency SIC Codes\n","threshold = 7  # Adjust this threshold based on your preference\n","\n","# Identify SIC Codes with frequency below the threshold\n","low_frequency_sic_codes = sic_code_frequency[sic_code_frequency < threshold].index\n","\n","# Replace these low-frequency SIC Codes with a common label \"Others\"\n","df['SIC Code'] = df['SIC Code'].replace(low_frequency_sic_codes, 'Others')\n","\n","sic_code_frequency1 = df['SIC Code'].value_counts()\n","\n","# Display the updated DataFrame\n","print(sic_code_frequency1)\n","\n","# Convert SIC Code to a 'category' data type so that it can be read by xgboost\n","df['SIC Code'] = df['SIC Code'].astype('category')"]},{"cell_type":"markdown","metadata":{"cell_id":"464099792d75487389d107c07dbabf96","deepnote_cell_type":"markdown"},"source":["* **1.1.7. Domain Knowledge**\n","\n","    We remove the column **`Fiscal Year End`** due to the following: We note that the objective is to predict annual sales figures, which implies all 12 months in a year will be evenly represented in the figure, regardless of when the fiscal year ends. Hence, the Fiscal Year End is irrelevant to the analysis."]},{"cell_type":"markdown","metadata":{"cell_id":"08de1765f90f44c0a2f727d4c4c03cca","deepnote_cell_type":"markdown"},"source":["* **1.1.8. Low Correlation to Target Variable** (or some feature selection to reduce dimensionality?)\n","\n","    With the understanding that correlation does not imply causation, we have chosen to remove columns in parts 1.1.1 to 1.1.7., regardless of the degree of correlation to the target variable, based on the justifications which have been laid out. \n","\n","    The attributes `Latitude`, `Longitude`, `SIC Code`, `Entity Type`, `Parent Country`, `Employees Domestic`, `Sales Domestic`, `Sales Global`, `Is Domestic Ultimate` will be included.\n","\n","    We seek here to remove ..."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7d92018e9de5447c8dc1463410ee9b89","deepnote_cell_type":"code"},"outputs":[],"source":["columns_to_remove = [\"AccountID\", \"Company\", \"8-Digit SIC Code\", \"8-Digit SIC Description\", \"Industry\", \"Company Description\", \"Company Status (Active/Inactive)\", \"Ownership Type\", \"Square Footage\", \"Parent Company\", \"Domestic Ultimate Company\", \"Global Ultimate Company\", \"Year Found\", \"Fiscal Year End\", \"Employees (Single Site)\", \"Employees (Global Ultimate Total)\", \"Import/Export Status\", \"Global Ultimate Country\", \"Is Global Ultimate\"]\n","df = df.drop(columns=columns_to_remove)"]},{"cell_type":"markdown","metadata":{"cell_id":"2073cd4d337241b49ec613c12564b03a","deepnote_cell_type":"markdown"},"source":["**1.2. Remove Irrelevant Rows**"]},{"cell_type":"markdown","metadata":{"cell_id":"8dac20c11df84d578c71128617db6d6f","deepnote_cell_type":"markdown"},"source":["Having decided the columns to retain for subsequent analysis, we remove rows with data deemed irrelevant due to them containing:"]},{"cell_type":"markdown","metadata":{"cell_id":"063f2712a795446b95071bb225119d0c","deepnote_cell_type":"markdown"},"source":["* **1.2.1. Missing Data and Imputation is Deemed Unnecessary**\n","\n","    We remove the rows with missing values for the `Latitude` and `Longitude` attributes. \n","\n","    We make this decision as we deem that rows for which `Latitude` and `Longitude` values are missing make up an insignificant part of the dataset. Exactly, they make up only 120 out of the 29,182 rows (according to 0.1. and 0.4.) (or 0.41% of the dataset). On the other hand, imputation would require that we derive the relative location of each of the entities and approximate the `Latitude` and `Longitude`, which, considering the significance to the dataset, is an unnecessarily cumbersome affair.\n","\n","    Additionally, we also remove the rows with missing values in the `Employees (Domestic Ultimate Total)` column. This is because there are 79 missing values out of the total 29,182 (which accounts for only 0.27% of the dataset)."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"d6351664bfb943ee82e553c63e0251bd","deepnote_cell_type":"code"},"outputs":[],"source":["# Remove rows without either Latitude or Longitude coordinates\n","df = df.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n","df = df.dropna(subset=[\"Employees (Domestic Ultimate Total)\"])"]},{"cell_type":"markdown","metadata":{"cell_id":"e07b14f68db54e5e9abd84367e32471e","deepnote_cell_type":"markdown"},"source":["* **1.2.2. Missing Data Unsuitable for Imputation (Imputation doesn't make sense)**\n","\n","    When we are unable to impute to deal with missing data in certain rows, the analysis with these rows will necessarily be incomplete. These rows should hence be removed to allow for complete analysis.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"64abab7f11084a2e95c6b75876997fd1","deepnote_cell_type":"markdown"},"source":["* **1.2.3. Outliers**\n","\n","    From the columns deemed relevant, perform linear regression or some statistical method to derive outliers. Conclude that outliers make up an insignificant part of dataset."]},{"cell_type":"markdown","metadata":{"cell_id":"10e1916eb0a74f339e7cf03d98a1e349","deepnote_cell_type":"markdown"},"source":["### **Part 2: Exploratory Data Analysis (EDA)**"]},{"cell_type":"markdown","metadata":{"cell_id":"014dcc73e1864a88a047157c1d727fbd","deepnote_cell_type":"markdown"},"source":["EDA is all about making sense of data in hand, before getting them dirty with any formal statistical analysis or modeling. It's a crucial step to understand the various aspects of the data you're working with, including the distribution, trends, and patterns that may exist. This process may include:\n","\n","* Data Summarization: Descriptive statistics like mean, median, mode, standard deviation, etc., are used to summarize the data.\n","* Handling Missing Data and Outliers: EDA also involves identifying and dealing with missing data points or outliers that could skew the analysis.\n","* Visualization: Graphical techniques, including histograms, box plots, scatter plots, etc., are employed extensively. These help in visualizing the distribution of the data, relationships between variables, and identifying patterns or anomalies.\n","* Identifying Patterns and Relationships: EDA often involves looking for correlations, trends, and patterns that can lead to meaningful insights.\n"]},{"cell_type":"markdown","metadata":{"cell_id":"3c59d5c7274f439f8a9e278ed0d34140","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["We will make use of a correlation matrix to observe how a numerical variable is affected by the other numerical variables in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f1b2c3e620e94f68a145a204f57fc5e8","deepnote_cell_type":"code"},"outputs":[],"source":["# use of a correlation matrix to observe how variables in our dataset may be affected by others\n","numerical_df = df2.select_dtypes(include=['float64', 'int64'])\n","\n","# Create a correlation matrix\n","correlation_matrix = numerical_df.corr()\n","\n","# Assuming correlation_matrix is your correlation matrix\n","correlation_df = pd.DataFrame(correlation_matrix)\n","\n","# Print or use the correlation DataFrame as needed\n","correlation_df"]},{"cell_type":"markdown","metadata":{"cell_id":"d168a765f3df4632a6022a77ad400252","deepnote_cell_type":"markdown"},"source":["**2.1. Data Summarization**"]},{"cell_type":"markdown","metadata":{"cell_id":"295b945e505f4f9dab48f9b2187bb192","deepnote_cell_type":"markdown"},"source":["This part has been covered under **0.1. General Information**"]},{"cell_type":"markdown","metadata":{"cell_id":"a98fe1b7da1346b0b0994b6c42e62bcf","deepnote_cell_type":"markdown"},"source":["**2.2. Outlier Analysis**\n","\n","We will be performing outlier analysis on `Sales Domestic` to identify anomalous data and remove them from our dataset. This is aimed at improving the accuracy of our model.\n","\n","We will use a boxplot to help us identify the anomalous data. Data points that are more than 3 interquartile range to the left of quartile 1 or right of quartile 3 will be considered as outliers and removed from our dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"f0625c8ec8b44c07a698fe44f104bb3a","deepnote_cell_type":"code"},"outputs":[],"source":["# Calculate the IQR\n","sales_col = 'Sales (Domestic Ultimate Total USD)'\n","Q1 = df[sales_col].quantile(0.25)\n","Q3 = df[sales_col].quantile(0.75)\n","IQR = Q3 - Q1\n","\n","# Define the lower and upper bounds for outliers\n","lower_bound = Q1 - 3 * IQR\n","upper_bound = Q3 + 3 * IQR\n","\n","# Extract outliers\n","outliers = df[(df[sales_col] < lower_bound) | (df[sales_col] > upper_bound)]\n","\n","# Remove rows with outliers\n","df = df[~((df[sales_col] < lower_bound) | (df[sales_col] > upper_bound))]\n","\n","#Plot boxplot\n","plt.figure(figsize=(10, 6))\n","sns.boxplot(x=df[sales_col], color='skyblue', width=0.5, linewidth=2)\n","plt.title('Boxplot of Sales', fontsize=16)\n","plt.xlabel('Sales (Domestic Ultimate Total USD)', fontsize=14)\n","plt.ylabel('Sales', fontsize=14)\n","plt.xticks(fontsize=12)\n","plt.yticks(fontsize=12)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"ddc626e1f98b474299861e9d1b183f88","deepnote_cell_type":"markdown"},"source":["**2.3. Data Visualization and Identifying Relationships**\n","\n","We will be performing EDA on `Latitude`, `Longitude`, `SIC Code`, `Entity Type`, `Parent Country`, `Employees Domestic`, `Sales Domestic`, `Sales Global`, `Is Domestic Ultimate`, which are the attributes we intend to keep, as outlined in 1.1.8:"]},{"cell_type":"markdown","metadata":{"cell_id":"e9a0d78312c14d1e8cde62d0b5b847ad","deepnote_cell_type":"markdown"},"source":["* **2.3.1 `Latitude` & `Longitude`**\n","\n","    We will perform a scatter plot to visualise coordinate data against sales."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e1de1207e5cd4979bd20288f11c0525d","deepnote_cell_type":"code"},"outputs":[],"source":["# import required libraries\n","import matplotlib.pyplot as plt\n","\n","# we will need this for the color gradient representing how high the domestic sales figure is.\n","from matplotlib.colors import LogNorm\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1581ce32588240f28d5371fc8b3e408f","deepnote_cell_type":"code"},"outputs":[],"source":["# for the upcoming geospatial analysis portion, we will use a small subset of the data provided for plotting.\n","filtered_df = df2[df2['SIC Code'].isin([5099, 1611, 8711, 4789])]\n","\n","filtered_df.head(2)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"725d8b01181e47cbb90346cbc01b459b","deepnote_cell_type":"code"},"outputs":[],"source":["# Use LogNorm for more sensitivity to numerical changes\n","norm = LogNorm()\n","\n","# Define a constant color (e.g., blue)\n","constant_color = 'blue'\n","\n","# Define a color palette based on unique industries\n","industry_palette = sns.color_palette('Set1', n_colors=len(filtered_df['Industry'].unique()))\n","\n","# Create a dictionary to map Industry to a unique color\n","industry_colors = dict(zip(filtered_df['Industry'].unique(), industry_palette))\n","\n","# Scatter plot with varying color by Industry and varying alpha based on Sales values\n","scatter = plt.scatter(filtered_df['LONGITUDE'], filtered_df['LATITUDE'], c=filtered_df['Industry'].map(industry_colors), s=3, alpha=norm(filtered_df['Sales (Domestic Ultimate Total USD)']))\n","\n","# Add a legend for Industry\n","legend_labels = filtered_df['Industry'].unique()\n","legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=industry,\n","                              markerfacecolor=industry_colors[industry], markersize=4) for industry in legend_labels]\n","plt.legend(handles=legend_handles, title='Industry', prop={'size': 5}, loc='upper left', bbox_to_anchor=(1, 1))\n","\n","# Set labels and title\n","plt.xlabel('Longitude')\n","plt.ylabel('Latitude')\n","plt.title('Scatter Plot with Varying Color by Industry and Varying Alpha Based on Sales (LogNorm)')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4a99cd815769433c88a7c27e7ac7c7d5","deepnote_cell_type":"code"},"outputs":[],"source":["# Use LogNorm for more sensitivity to numerical changes\n","norm = LogNorm()\n","\n","# Define a colormap transitioning from blue to red\n","cmap = 'plasma'\n","\n","# Scatter plot with colors based on Sales values and LogNorm\n","scatter = plt.scatter(filtered_df['LONGITUDE'], filtered_df['LATITUDE'], c=filtered_df['Sales (Domestic Ultimate Total USD)'], cmap=cmap, s=5, norm=norm)\n","\n","# Add a colorbar\n","cbar = plt.colorbar(scatter, label='Sales')\n","\n","# Set labels and title\n","plt.xlabel('Longitude')\n","plt.ylabel('Latitude')\n","plt.title('Scatter Plot with Colors Based on Sales (LogNorm)')\n","\n","# Show the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"8b0d89c5b09245e195742f1b6d0252fa","deepnote_cell_type":"markdown"},"source":["* **2.3.2 `SIC Code`**\n","\n","    Here we plot a bar graph to visualise the SIC codes by sales."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"0578935b0a8241b0ac2ec35d307f8636","deepnote_cell_type":"code"},"outputs":[],"source":["# Sort the data by revenue in descending order\n","sales_by_sic = df.groupby('SIC Code')['Sales (Domestic Ultimate Total USD)'].sum().sort_values(ascending=False)\n","\n","# Select the top 10 SIC codes\n","top_10_sic = sales_by_sic.head(10)\n","\n","# Plot the data\n","plt.figure(figsize=(12, 6))\n","top_10_sic.plot(kind='bar')\n","plt.xlabel('SIC Code')\n","plt.ylabel('Sales (Domestic Ultimate Total USD)')\n","plt.title('Top 10 Sales by SIC Code')\n","plt.xticks(rotation=45)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"cd21365269f24ca1bd1eb8e485699442","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["We see that the companies with different SIC Codes do have varying amounts of sales, therefore we keep this attribute."]},{"cell_type":"markdown","metadata":{"cell_id":"5ca252477fa7478e82b4c68dfdb0a045","deepnote_cell_type":"markdown"},"source":["* **2.3.3 `Entity Type`**\n","\n","    Here we will perform a boxplot and a ANOVA test to understand the different entity types better, and to visualise the correlation between `Entity Type` and `Sales (Domestic Ultimate Total USD)`."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"ffb912ee08ea4e80a84c65e99e9f8b6f","deepnote_cell_type":"code"},"outputs":[],"source":["plt.figure(figsize=(10, 6))\n","sns.boxplot(x='Entity Type', y='Sales (Domestic Ultimate Total USD)', data=df2)\n","plt.xticks(rotation=90)  # Rotate x-axis labels for readability\n","plt.title('Entity Type vs Sales')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"8432f84d914144dd8beb9f9ab49c7d7a","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["From the above boxplot, we can see that the variance and maximum values increase in the order of 'Branch', 'Independent', 'Parent', 'Subsidiary'."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"fe1acaaac99c4997a357e3aea5f1116c","deepnote_cell_type":"code"},"outputs":[],"source":["from scipy import stats\n","\n","# Get a list of entity types\n","entity_types = df2['Entity Type'].unique()\n","\n","# Create a list of sales for each entity type\n","sales_by_entity_type = [df2.loc[df2['Entity Type'] == entity_type, 'Sales (Domestic Ultimate Total USD)'] for entity_type in entity_types]\n","\n","# Perform ANOVA test\n","f_val, p_val = stats.f_oneway(*sales_by_entity_type)\n","print(f\"F-value: {f_val}, p-value: {p_val}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"a786ffca151a46599a5fa72f068b23da","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The interpretation of the ANOVA test is as shown below."]},{"cell_type":"markdown","metadata":{"cell_id":"4f4592f1ca5c43b09520ea5b4104c4c8","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The F-value of 14.80725389236939 is a measure of how much the means of `Sales (Domestic Ultimate Total USD)` for each `Entity Type` vary between the groups compared to how much they vary within the groups. A high F-value indicates that the means are significantly different."]},{"cell_type":"markdown","metadata":{"cell_id":"601ab4c6d6664434ae96350295f40294","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["The p-value of 1.2478331694905712e-09 is very small, much less than the typical threshold of 0.05. This indicates strong evidence against the null hypothesis. The null hypothesis for an ANOVA test is that all means are equal. Therefore, you can reject the null hypothesis and conclude that there is a statistically significant difference in the means of `Sales (Domestic Ultimate Total USD)` between the different `Entity Type`s."]},{"cell_type":"markdown","metadata":{"cell_id":"1733272a86774698956e4428138640b0","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["In other words, the type of entity appears to have a significant effect on sales. "]},{"cell_type":"markdown","metadata":{"cell_id":"276a5f3772204fb7873c03c7adff11ac","deepnote_cell_type":"markdown"},"source":["* **2.3.4 `Parent Country`**\n","\n","    Here we will perform a barplot and a ANOVA test to understand the different parent country better, and to visualise the correlation between `Parent Country` and `Sales (Domestic Ultimate Total USD)`."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"e19b341c42074a9cac4e5d3e14f10b09","deepnote_cell_type":"code"},"outputs":[],"source":["plt.figure(figsize=(15, 20))\n","sns.barplot(data=df2, y='Parent Country', x='Sales (Domestic Ultimate Total USD)')\n","plt.title('Horizontal Bar Plot of Sales by Parent Country')\n","plt.xlabel('Sales')\n","plt.ylabel('Parent Country')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"cell_id":"f54df77d1d53422ab2061666161c9067","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["From the above bar chart, we can see that different country of the parent company does affect the amount of sales."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"a28623e8ca67423183c60e26cf66f61e","deepnote_cell_type":"code"},"outputs":[],"source":["# Replace missing values in \"Parent Country\" with \"Unknown\"\n","df2['Parent Country'].fillna('Unknown', inplace=True)\n","\n","# Get a list of parent country\n","parent_countries = df2['Parent Country'].unique()\n","\n","# Create a list of sales for each entity type\n","sales_by_parent_country = [df2.loc[df2['Parent Country'] == parent_country, 'Sales (Domestic Ultimate Total USD)'].values for parent_country in parent_countries]\n","\n","# Perform ANOVA test\n","f_val, p_val = stats.f_oneway(*sales_by_parent_country)\n","print(f\"F-value: {f_val}, p-value: {p_val}\")"]},{"cell_type":"markdown","metadata":{"cell_id":"ea8ed2842d6846269b7cb28eb20162d7","deepnote_cell_type":"markdown"},"source":["The F-value of 6.950732217639041 is a measure of how much the means of `Sales (Domestic Ultimate Total USD)` for each `Parent Country` vary between the groups compared to how much they vary within the groups. A high F-value indicates that the means are significantly different."]},{"cell_type":"markdown","metadata":{"cell_id":"0c8acb551f674ce7871dab5212eafe1a","deepnote_cell_type":"markdown"},"source":["* **2.3.5 `Employees Domestic`**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7ad877c048dc4ed3a9ae21512884b282","deepnote_cell_type":"code"},"outputs":[],"source":["sns.scatterplot(x='Employees (Domestic Ultimate Total)', y='Sales (Domestic Ultimate Total USD)', data=df)"]},{"cell_type":"markdown","metadata":{"cell_id":"20cd0a5a016a43648da375214d520c17","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["From here, we notice that there is a negative correlation between domestic sales and the total number of individuals employed by a company and hence, we will include this feature variable."]},{"cell_type":"markdown","metadata":{"cell_id":"d7c5446df605400298cbfacb491c6fd1","deepnote_cell_type":"markdown"},"source":["* **2.3.6 `Sales Global`**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"aa5d94bed8354d1b83b546e501bb981d","deepnote_cell_type":"code"},"outputs":[],"source":["filtered_df = df[df['Sales (Global Ultimate Total USD)'] != df['Sales (Domestic Ultimate Total USD)']]\n","sns.scatterplot(x='Sales (Global Ultimate Total USD)', y='Sales (Domestic Ultimate Total USD)', data=filtered_df)"]},{"cell_type":"markdown","metadata":{"cell_id":"5395064dc76b4ac99a85ba6476824602","deepnote_cell_type":"text-cell-p","formattedRanges":[]},"source":["From here, we notice that there is a negative correlation between domestic sales and global sales and hence, we will include this feature variable."]},{"cell_type":"markdown","metadata":{"cell_id":"a106249026df474db2809c7c1b3ee6cf","deepnote_cell_type":"markdown"},"source":["* **2.3.7 `Is Domestic Ultimate`**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7f38774c0a6b4a85a769c50147770103","deepnote_cell_type":"code"},"outputs":[],"source":["sns.boxplot(x='Is Domestic Ultimate', y='Sales (Domestic Ultimate Total USD)', data=df)\n","plt.xlabel('Is Domestic Ultimate')\n","plt.ylabel('Sales (Domestic Ultimate Total USD)')"]},{"cell_type":"markdown","metadata":{"cell_id":"2e280e157eff4e57b68c4d4a2e31465f","deepnote_cell_type":"markdown"},"source":["From here, we notice that there is a relationship between `Is Domestic Ultimate` and `Sales (Domestic Ultimate Total USD)`. When company is the ultimate or highest-level company within a corporate structure based in its home country, domestic sales tend to be slightly higher on average."]},{"cell_type":"markdown","metadata":{"cell_id":"31a31ca282eb40e194f7c23b4562a844","deepnote_cell_type":"markdown"},"source":["### **Part 3: Model Selection**"]},{"cell_type":"markdown","metadata":{"cell_id":"b2a2354ea4564af9b161978cd6b2bd6d","deepnote_cell_type":"markdown"},"source":["We chose XGBoost as our predictive modeling tool for several compelling reasons. XGBoost, which stands for eXtreme Gradient Boosting, is renowned for its efficiency and effectiveness in solving classification and regression problems.\n","\n"," * Performance: XGBoost is known for delivering high-performance models. It has been the algorithm behind many winning solutions in Kaggle competitions, which attests to its capability in handling a wide range of data science problems effectively.\n","\n"," * Handling of Large Datasets: XGBoost is optimized to efficiently handle large datasets. Its scalability in both speed and performance is a critical factor, especially in a datathon setting where time and resources are limited.\n","\n"," * Versatility: This model can handle a variety of data types, relationships, and distributions, making it a versatile choice for our diverse datasets. It also works well with both categorical and numerical inputs, which are common in many real-world datasets.\n","\n"," * Robustness to Overfitting: XGBoost includes built-in regularization, which helps prevent overfitting. This is crucial in ensuring that our model generalizes well to unseen data.\n","\n"," * Feature Importance: XGBoost provides useful insights into the importance of each feature in the training process, allowing us to understand which factors are driving the predictions. This interpretability is valuable in making data-driven decisions and presentations.\n","\n","In sum, XGBoost offers a powerful combination of speed, performance, and flexibility, making it an excellent choice for our datathon's predictive modeling tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"1b5c0a31b17d4c0f8c3eae4d2c269265","deepnote_cell_type":"code"},"outputs":[],"source":["# import scikit learn packages\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","# import xgboost\n","import xgboost as xgb"]},{"cell_type":"markdown","metadata":{"cell_id":"0fdb7f5a234c4cd49268c374987b357f","deepnote_cell_type":"markdown"},"source":["### **Part 4: Model Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"2ca6bcba50f94b9a8b22d374e46e079e","deepnote_cell_type":"code"},"outputs":[],"source":["from sklearn.model_selection import train_test_split, GridSearchCV\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","\n","# Extract features (X) and target variable (y)\n","X = df.drop(columns=['Sales (Domestic Ultimate Total USD)'])\n","y = df['Sales (Domestic Ultimate Total USD)']\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Initialize the XGBoost regressor\n","model = XGBRegressor(enable_categorical=True)\n","\n","# Define hyperparameters grid for grid search\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'min_child_weight': [1, 3, 5],\n","    'gamma': [0.0, 0.1, 0.2],\n","    'subsample': [0.6, 0.8, 1.0],\n","    'colsample_bytree': [0.6, 0.8, 1.0],\n","}\n","\n","# Perform grid search with cross-validation\n","grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')\n","grid_search.fit(X_train, y_train)\n","\n","# Get the best model from grid search\n","best_model = grid_search.best_estimator_\n","\n","# Make predictions on the testing data\n","y_pred = best_model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"90863aec13604b42bed0e740a7897bcf","deepnote_cell_type":"code"},"outputs":[],"source":["# Save the model\n","best_model.save_model('7UP_xgboost_model.json')"]},{"cell_type":"markdown","metadata":{"cell_id":"20d801251bf04f25a4aa9402a3f93989","deepnote_cell_type":"markdown"},"source":["### **Part 5: Model Evaluation**"]},{"cell_type":"markdown","metadata":{"cell_id":"e699046fea1940069811d4aa2bda49d0","deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":40,"marks":{"bold":true},"toCodePoint":58,"type":"marks"},{"fromCodePoint":67,"marks":{"bold":true},"toCodePoint":97,"type":"marks"},{"fromCodePoint":103,"marks":{"bold":true},"toCodePoint":119,"type":"marks"}]},"source":["To evaluate our model, we will be using evaluation metrics such as Root Mean Squared Error (RMSE), and R-squared value."]},{"cell_type":"markdown","metadata":{"cell_id":"5e471e9c14c340bdbf85b05951f778f4","deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":5,"type":"marks"}]},"source":["RMSE is the average of the squared differences between the predicted and actual values."]},{"cell_type":"markdown","metadata":{"cell_id":"6333aa050a874214a5ffe45237f7207c","deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":15,"type":"marks"}]},"source":["R-squared Value is the proportion of the variance in the target variable explained by the model."]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"0ef7d184205146e6acac7f297494df49","deepnote_cell_type":"code"},"outputs":[],"source":["# Calculate the Root Mean Squared Error (RMSE)\n","rmse = mean_squared_error(y_test, y_pred, squared=False)\n","print(\"Root Mean Squared Error:\", rmse)\n","\n","#Calculate the R-squared Value\n","r_squared = r2_score(y_test, y_pred)\n","print(\"R-squared:\", r_squared)"]},{"cell_type":"markdown","metadata":{"cell_id":"3e59eb9b21294de4b6f704ae9bda7c18","deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":40,"marks":{"bold":true},"toCodePoint":65,"type":"marks"},{"fromCodePoint":70,"marks":{"bold":true},"toCodePoint":107,"type":"marks"}]},"source":["From our model, we managed to obtain an RMSE of 9176994.121846402 and R-squared value of 0.6343294375554753 which is an indication that the model is decently accurate in predicting the actual value."]},{"cell_type":"markdown","metadata":{"cell_id":"991d953b084546b49b7e9302fbb19bab","deepnote_cell_type":"markdown"},"source":["## The cell below is **NOT** to be removed\n","##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n","##### It is recommended to test the function out prior to submission\n","-------------------------------------------------------------------------------------------------------------------------------\n","##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n","##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"]},{"cell_type":"code","execution_count":3,"metadata":{"cell_id":"0d0eb445315d476888becdd5544fe3f9","deepnote_cell_type":"code"},"outputs":[],"source":["# Import relevant packages\n","import pandas as pd\n","from xgboost import XGBRegressor\n","\n","def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n","    '''DO NOT REMOVE THIS FUNCTION.\n","\n","The function accepts a dataframe as input and return an iterable (list)\n","of binary classes as output.\n","\n","The function should be coded to test on hidden data\n","and should include any preprocessing functions needed for your model to perform. \n","    \n","All relevant code MUST be included in this function.'''\n","\n","    ## Load Model Function\n","    def load_model(filename):\n","        # Load the model\n","        loaded_model = XGBRegressor()\n","        loaded_model.load_model('7UP_xgboost_model.json')\n","        return loaded_model\n","\n","    ## Preprocessing Function\n","\n","    def preprocess(data: pd.DataFrame):\n","\n","        ## One-hot encoding\n","        data = pd.get_dummies(data, columns=['Entity Type'], prefix = 'Entity Type')\n","        data = pd.get_dummies(data, columns=['Parent Country'], prefix='Parent Country')\n","\n","        ## SIC Code handling\n","\n","        # Find the frequency of each industry\n","        sic_code_frequency = df['SIC Code'].value_counts()\n","        # Note, use original df here so the same list of SIC codes are used as in the xgboost model.\n","\n","        # Set a threshold for low-frequency SIC Codes\n","        threshold = 7  # Adjust this threshold based on your preference\n","\n","        # Identify SIC Codes with frequency below the threshold\n","        low_frequency_sic_codes = sic_code_frequency[sic_code_frequency < threshold].index\n","\n","        # Replace these low-frequency SIC Codes with a common label \"Others\"\n","        data['SIC Code'] = data['SIC Code'].replace(low_frequency_sic_codes, 'Others')\n","\n","        sic_code_frequency1 = data['SIC Code'].value_counts()\n","\n","        # Display the updated DataFrame\n","        print(sic_code_frequency1)\n","\n","        # Convert SIC Code to a 'category' data type so that it can be read by xgboost\n","        data['SIC Code'] = data['SIC Code'].astype('category')\n","\n","        ## Column removal\n","        columns_to_remove = [\"AccountID\", \"Company\", \"8-Digit SIC Code\", \"8-Digit SIC Description\", \"Industry\", \"Company Description\", \"Company Status (Active/Inactive)\", \"Ownership Type\", \"Square Footage\", \"Parent Company\", \"Domestic Ultimate Company\", \"Global Ultimate Company\", \"Year Found\", \"Fiscal Year End\", \"Employees (Single Site)\", \"Employees (Global Ultimate Total)\", \"Import/Export Status\", \"Global Ultimate Country\", \"Is Global Ultimate\"]\n","        data = data.drop(columns=columns_to_remove)\n","\n","        ## Latitude or Longitude NA value handling\n","\n","        # Remove rows without either Latitude or Longitude coordinates\n","        data = data.dropna(subset=[\"LATITUDE\", \"LONGITUDE\"])\n","        data = data.dropna(subset=[\"Employees (Domestic Ultimate Total)\"])\n","\n","        return data\n","\n","\n","    # Preprocess data\n","    hidden_data = preprocess(hidden_data)\n","\n","    # Load model\n","    model = load_model\n","\n","    # Make predictions\n","    predictions = model.predict(hidden_data)\n","\n","    # Convert to list\n","    result = predictions.tolist()\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"cell_id":"580118a7c9d1423fb2e5cd7946cf76ae","deepnote_cell_type":"markdown"},"source":["##### Cell to check testing_hidden_data function"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"15c1776b234a406780f4f4700c7188b8","deepnote_cell_type":"code"},"outputs":[],"source":["# This cell should output a list of predictions.\n","test_df = pd.read_csv(filepath)\n","test_df = test_df.drop(columns=['Sales (Domestic Ultimate Total USD)'])\n","print(testing_hidden_data(test_df))"]},{"cell_type":"markdown","metadata":{"cell_id":"662acf560277455f8f151e4d5b736470","deepnote_cell_type":"markdown"},"source":["### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"]}],"metadata":{"deepnote_execution_queue":[],"deepnote_notebook_id":"ff99fff804984a26a2fd5dab550df34b","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
